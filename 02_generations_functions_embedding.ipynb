{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ae715a-9646-42b2-be7f-b490e3ad17a8",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fce514-793d-40c4-9760-d5c3709eebe2",
   "metadata": {
    "tags": []
   },
   "source": [
    "The latest models, gpt-4 (and gpt-4 turbo) and gpt-3.5-turbo, are accessed through the **chat completions API** endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c680892-8e34-47ed-8762-cd2c7c950b73",
   "metadata": {},
   "source": [
    "## Chat Completion API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c8c4a-eaf3-44c2-b2e4-c1298b48361f",
   "metadata": {},
   "source": [
    "Chat completion APIs are good for both: \n",
    "* multi-turn conversations, \n",
    "* and single task completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b64ade0-9ccf-4262-818b-5cfabf7f4a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I don't know!\"},  #\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Who cares?\"},  #\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "# Looks like the provided example assistant answers have no impact on the output response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6ab266d-09ca-4c2e-adbb-2e90824d86dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The 2020 World Series was played at the Globe Life Field in Arlington, Texas.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617adbee-cf92-497b-85f8-78bdb74bebae",
   "metadata": {},
   "source": [
    "The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the model’s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
    "\n",
    "The user messages provide requests or comments for the assistant to respond to. Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
    "\n",
    "Including conversation history is important when user instructions refer to prior messages. In the example above, the user’s final question of \"Where was it played?\" only makes sense in the context of the prior messages about the World Series of 2020. Because the models have no memory of past requests, all relevant information must be supplied as part of the conversation history in each request. If a conversation cannot fit within the model’s token limit, it will need to be shortened in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe9734-32e7-4f5a-a590-e576b495e0a1",
   "metadata": {},
   "source": [
    "## JSON mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7367931-aa41-4a6a-aa17-c91033d44f2d",
   "metadata": {},
   "source": [
    "You can set response_format to `{ \"type\": \"json_object\" }` to enable JSON mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b291e-ac67-4c4a-b1a3-6b48d1064074",
   "metadata": {},
   "source": [
    "* When using JSON mode, **always** ensure to instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don't include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don't forget, the API will throw an error if the string \"JSON\" does not appear somewhere in the context.\n",
    "* The JSON in the message the model returns may be partial (i.e. cut off) if finish_reason is length, which indicates the generation exceeded max_tokens or the conversation exceeded the token limit. To guard against this, check finish_reason before parsing the response.\n",
    "* JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad849a71-65f5-4de1-b2e9-5e4d0889221c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"winner\": \"Los Angeles Dodgers\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  response_format={ \"type\": \"json_object\" },\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a8a9fe2-d45e-4394-b638-99c04948ee92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"winner\": \"Los Angeles Dodgers\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4ae80-baa4-461b-a37a-fc7c4f2085a3",
   "metadata": {},
   "source": [
    "## Reproducible Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4eddb-a031-46fc-ac79-090c94a418b2",
   "metadata": {},
   "source": [
    "You can use **seed** and also potentially **system_fingerprints**:\n",
    "https://platform.openai.com/docs/guides/text-generation/reproducible-outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed92c55-be4f-4f87-843b-b4dfa5cdab22",
   "metadata": {},
   "source": [
    "## Managing Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01dd62b-0264-4aa0-8fdb-903fc62318ba",
   "metadata": {},
   "source": [
    "For example, the string **\"ChatGPT is great!\"** is encoded into six tokens: [\"Chat\", \"G\", \"PT\", **\" is\"**, **\" great\"**, \"!\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca43bee-d142-4443-b7fc-2e53e70ab98e",
   "metadata": {},
   "source": [
    "### Token Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc2aca-b2dd-4caa-84c1-96e40bf5ba25",
   "metadata": {},
   "source": [
    "To see how many tokens are used by an API call, check the usage field in the API response (e.g., response['usage']['total_tokens'])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fe8ff1d-abe3-40ed-9542-4850f074965c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=11, prompt_tokens=31, total_tokens=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382888c3-0a73-47ba-8887-977bcccd917f",
   "metadata": {},
   "source": [
    "To see how many tokens are in a text string without making an API call, use OpenAI’s [tiktoken](https://github.com/openai/tiktoken) Python library. Example code can be found in the OpenAI Cookbook’s guide on [how to count tokens with tiktoken](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken).\n",
    "\n",
    "Each message passed to the API consumes the number of tokens in the content, role, and other fields, plus a few extra for behind-the-scenes formatting. This may change slightly in the future.\n",
    "\n",
    "If a conversation has too many tokens to fit within a model’s maximum limit (e.g., more than 4097 tokens for gpt-3.5-turbo), you will have to truncate, omit, or otherwise shrink your text until it fits. Beware that if a message is removed from the messages input, the model will lose all knowledge of it.\n",
    "\n",
    "Note that very long conversations are more likely to receive incomplete replies. For example, a gpt-3.5-turbo conversation that is 4090 tokens long will have its reply cut off after just 6 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76525510-582b-4435-aab0-28161e4d87ef",
   "metadata": {},
   "source": [
    "## How should I set the temperature parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dff4b6-6503-452a-a36f-836586eb4ae6",
   "metadata": {},
   "source": [
    "The temperature can range is from 0 to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24613c95-9a6f-4db7-b2de-dc08d7a63e56",
   "metadata": {},
   "source": [
    "# Function Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d9415-ac42-4cad-83e8-fd3bb5908621",
   "metadata": {
    "tags": []
   },
   "source": [
    "Connect LLMs to external tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63bf97-4ada-4a80-8305-881aaa9e0817",
   "metadata": {},
   "source": [
    "In an API call, you can **describe** functions and have the model intelligently choose to **output a JSON object containing arguments** to call one or many functions. \n",
    "\n",
    "The Chat Completions API **does not call** the function; instead, the model **generates JSON** that you can use to **call the function** in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a6a4e-9ab2-491b-9567-5c7df5c3aee1",
   "metadata": {
    "tags": []
   },
   "source": [
    "https://platform.openai.com/docs/guides/function-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9761c02d-c283-4476-aec4-b8bedc389bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    if response_message.content is None:\n",
    "        response_message.content = \"\"\n",
    "    if response_message.function_call is None:\n",
    "        del response_message.function_call\n",
    "    tool_calls = response_message.tool_calls\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                location=function_args.get(\"location\"),\n",
    "                unit=function_args.get(\"unit\"),\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "        return second_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91268fa3-5529-4c3e-99ff-fb7ca13e05a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8JZTTmpyMEYWQkT4Px8OCgKBbT69F', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Currently, the weather in San Francisco is 72°F and partly cloudy, in Tokyo it's 10°C and cloudy, and in Paris it's 22°C and sunny.\", role='assistant', function_call=None, tool_calls=None))], created=1699675287, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=36, prompt_tokens=169, total_tokens=205))\n"
     ]
    }
   ],
   "source": [
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bccbad-7143-4456-8b94-04fa151956b3",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51488cbc-fd6a-48c2-8004-498415d58c24",
   "metadata": {},
   "source": [
    "OpenAI’s text embeddings measure the relatedness of text strings. \n",
    "\n",
    "It is recommended to use text-embedding-ada-002 for nearly all use cases. It’s better, cheaper, and simpler to use. Read the [blog post announcement](https://openai.com/blog/new-and-improved-embedding-model).\n",
    "\n",
    "* Search (where results are ranked by relevance to a query string)\n",
    "* Clustering (where text strings are grouped by similarity)\n",
    "* Recommendations (where items with related text strings are recommended)\n",
    "* Anomaly detection (where outliers with little relatedness are identified)\n",
    "* Diversity measurement (where similarity distributions are analyzed)\n",
    "* Classification (where text strings are classified by their most similar label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee1427-8c7f-4560-9af9-80a8f288a3dc",
   "metadata": {},
   "source": [
    "### Example Code snippets for above Embedding Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342c9aa-9be2-4ba7-a8cb-65a34a7ff32c",
   "metadata": {
    "tags": []
   },
   "source": [
    "https://platform.openai.com/docs/guides/embeddings/use-cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85fd22-6813-490d-96ab-f54c6b365daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
